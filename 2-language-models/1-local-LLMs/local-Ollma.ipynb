{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4cfaad",
   "metadata": {},
   "source": [
    "## Running LLMs Locally\n",
    "In this lab we'll use [Ollama](https://ollama.com/) do run LLMs locally.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6caa3",
   "metadata": {},
   "source": [
    "### Running LLMs locally - the basics\n",
    "- Intelligence:\n",
    "  - 1B - 3B: Great for basic summaries or running on a phone.\n",
    "  - 7B - 14B: The \"sweet spot\" for laptops. Smart enough to follow complex instructions and code.\n",
    "  - 70B+: Extremely smart (near GPT-4 level), but usually requires a powerful desktop or server.\n",
    "- Memory consumption:\n",
    "  - Parameters start as float32, so 4 butes per parmeter\n",
    "  - But they are compresses, ussing lossy compression (quantization, like JPEG/MPEG)\n",
    "  - An original float32 becomes INt4 (a nibble - half a byte)\n",
    "  - An 8b model typically needs about 5GB to 8GB of RAM to run\n",
    "- Where is the memory:\n",
    "  - If you have a GPU - Ollama will use the VRAM (Video RAM) of the GPU\n",
    "  - Use **nvidia-smi** command to see if you have an Nvidia GPU and how much VRAM it has\n",
    "  - use **ollama ps** to see what model(s) are loaded into RAM/VRAM\n",
    "  - Ollama is limited by the environment variable OLLAMA_MAX_LOADED_MODELS (default is 3) and, more importantly, your physical hardware limits.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3ca8e",
   "metadata": {},
   "source": [
    "### Hands-On: install Ollama and run a model\n",
    "\n",
    "- Go to ollama.com and download the installer for your OS (Windows, macOS, or Linux).  \n",
    "The installation file includes just the engine, you'll have to download models later.\n",
    "- Run the installer. You will see a small llama icon in your system tray once it's running.\n",
    "- Download and run your first model:\n",
    "  - **ollama run llama3.1:8b**  \n",
    "  (for version 3.1 )\n",
    "  - Or you can select the model from the tool and if it is not downloaded = it will"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
