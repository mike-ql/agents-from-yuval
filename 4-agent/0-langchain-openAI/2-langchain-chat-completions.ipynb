{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d70ebd",
   "metadata": {},
   "source": [
    "## LangChain OpenAI -  chat-completion API\n",
    "- We demonstrate:\n",
    "  - Using a chain (the legacy way)\n",
    "  - LangGraph behind the scenes (The new way using create_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d52372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# For the new way to use chat-completion:\n",
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d3b44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Model\n",
    "# Note: Ensure OPENAI_API_KEY is in your environment variables\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b785bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a witty and helpful technical assistant named Gemini's Peer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "485863fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lazy evaluation is a programming technique where expressions are not evaluated until their values are needed, improving efficiency by avoiding unnecessary calculations.\n"
     ]
    }
   ],
   "source": [
    "# 3. Legacy way - Create a Chain\n",
    "# We use StrOutputParser to get back a string instead of a full ChatMessage object\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# 4. Invoke the Chain\n",
    "response = chain.invoke({\"input\": \"Explain the concept of 'lazy evaluation' in one sentence.\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fb4ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages (without the system message): 2.\n",
      "Messages: content=\"What's the weather like in London?\" additional_kwargs={} response_metadata={} id='8ebe7f7c-16ac-46cb-b3da-e8d3e5d45ce1'\n",
      "Messages: content=\"I currently don't have access to real-time data or the ability to retrieve current weather conditions. To get the most up-to-date weather information for London, I recommend checking a reliable weather website or a weather app on your smartphone. They can provide you with current conditions, forecasts, and any weather alerts for the area.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 25, 'total_tokens': 88, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-D96k7IyTvzzy7vRnAvpdRfPSBZHzF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c5b98-75e9-7660-9c48-121c3a1b949a-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 25, 'output_tokens': 63, 'total_tokens': 88, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 4. Demo using the create_agent, SO NO CHAINS and LangGraph under the hood - even if I don't import it directly\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Create the agent\n",
    "# This is the \"chainless\" way: no pipe operators (|)\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    system_prompt=\"You are a helpful weather assistant.\"\n",
    ")\n",
    "\n",
    "# Invoke it\n",
    "# create_agent returns an object with a simple .invoke() interface\n",
    "result = agent.invoke({\n",
    "    \"messages\": [(\"user\", \"What's the weather like in London?\")]\n",
    "})\n",
    "\n",
    "# Access the final response\n",
    "# Note that one messahe is missing here: The system message. It is sent to the LLM, but not included in the response.\n",
    "print(f'Number of messages (without the system message): {len(result[\"messages\"])}.')\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f'Messages: {msg}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
